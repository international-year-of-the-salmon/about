---
title: "Data"
---

Datasets from the IYS expeditions are in various stages of being published. A dataset status number (1-5) is assgned to datasets and a FAIR (Findable, Accessible, Interoperable, Reusable) badge is assigned when the data reach level 5.

Dataset status levels:

- Level 1: Data only have a metadata record, no data have been received. 
- Level 2: Data have a metadata record, data have been received and are available internally to IYS expedition scientists but not yet published publicly. 
- Level 3: Data have a metadata record and only a subset of the data have been standardized and made publicly available in a domain-specific repository. 
- Level 4: Data have a metadata record and a non-standardized version of the dataset is publicly available.
- Level 5: Data have a metadata record and the full fidelity of the dataset is standardized and available in a domain-specific repository.



```{r setup, include = FALSE}
knitr::opts_chunk$set(include = FALSE, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(reactable)
library(googlesheets4)

```

```{r, include = FALSE}
# ckanr_setup(url = "http://iys.hakai.org")
# pkgs <- package_list_current(as = "table", limit = 2000)
# 
# names(pkgs)
# 
# pkgs <- pkgs %>%
#   mutate(`metadata-point-of-contact` = map(`metadata-point-of-contact`, ~select(.x, c('individual-name', 'contact-info_email'))))
# 
# 
# subset <- pkgs %>%
#   select(title, id = name, `metadata-point-of-contact`, `unique-resource-identifier-full`, extras) %>%
#   filter(`unique-resource-identifier-full` != "NULL")
# 
# constraints <- bind_rows(subset$extras) %>%
#   filter(key == "licence")
# 
# names <- bind_rows(subset$`metadata-point-of-contact`, .id = "Name") %>%
#   distinct(Name, .keep_all = TRUE) %>%
#   select(`Contact Person` = 'individual-name',
#          `Contact email` = 'contact-info_email')
# 
# dois <- bind_rows(subset$`unique-resource-identifier-full`) %>%
#   mutate(dois = paste0("https://doi.org/", code)) %>%
#   select(`Digital Object Identifier`= dois)
# 
# subset <- tibble(`Dataset Title` = subset$title, names, dois, `Access Constraints` = constraints$value) %>%
#   mutate("FAIRness" = if_else(`Access Constraints` == "", 5,4))
# 
# write_csv(subset, here::here("published.csv"))
# #At this stage I read the sheet into google sheets to make manual edits
# not_published <- pkgs %>%
#   select(title, id = name, `metadata-point-of-contact`, `unique-resource-identifier-full`) %>%
#   filter(`unique-resource-identifier-full` == "NULL") %>%
#   mutate(`Metadata Link` = paste0("https://iys.hakai.org/dataset/", id))
# 
# names <- bind_rows(not_published$`metadata-point-of-contact`, .id = "Name") %>%
#   distinct(Name, .keep_all = TRUE) %>%
#   select(`Contact Person` = 'individual-name',
#          `Contact email` = 'contact-info_email')
# 
# not_published <- tibble(`Dataset Title` = not_published$title, names,
#                         `Metadata Link` = not_published$`Metadata Link`) %>%
#   mutate("FAIRness" = 1)
# 
# write_csv(not_published, here::here('not_published.csv'))
```

```{r 2019 and 2020 data, include = FALSE}
gs4_deauth()
#(`Dataset Title`,"\\(?[0-9,.]+\\)?")[[1]],
published_data <- read_sheet("https://docs.google.com/spreadsheets/d/1SoeRT_eytuMS3-Ap5q6UPzcwBK2HKiqzsTbWLmxZcBA/edit#gid=1459056304", sheet = "Published")

published_data$Year <- str_match_all(published_data$`Dataset Title`, "[0-9]+")

published_data <- published_data %>% 
  mutate(Vessel = if_else(Year == "2019", "Professor Kaganovsky", "Pacific Legacy NO 1"),
         Vessel = if_else(Year == 'c("2019", "2020")', "Professor Kaganovsky, Pacific Legacy NO 1", Vessel),
         Year = as.numeric(ifelse(Year == 'c("2019", "2020")', "20192020", Year))) %>% 
  select(`Dataset Title`, Year, Vessel, `Contact Person`, `Contact email`, `Link to Access Data` = `Digital Object Identifier`, `Access Constraints`:FAIRness)


internal_data <- read_sheet("https://docs.google.com/spreadsheets/d/1SoeRT_eytuMS3-Ap5q6UPzcwBK2HKiqzsTbWLmxZcBA/edit#gid=1459056304", sheet = "Available Internally") 

internal_data$Year <- str_match_all(internal_data$`Dataset Title`, "[0-9]+")

internal_data <- internal_data %>% 
  mutate(Vessel = if_else(Year == "2019", "Professor Kaganovsky", "Pacific Legacy NO 1"),
         Vessel = if_else(Year == 'c("2019", "2020")', "Professor Kaganovsky, Pacific Legacy NO 1", Vessel),
         Year = as.numeric(ifelse(Year == 'c("2019", "2020")', "20192020", Year))) %>% 
  select(`Dataset Title`, Year, Vessel, `Contact Person`, `Contact email`, `Link to Access Data` = `Metadata Link`, Status)

unpublished_data <- read_sheet("https://docs.google.com/spreadsheets/d/1SoeRT_eytuMS3-Ap5q6UPzcwBK2HKiqzsTbWLmxZcBA/edit#gid=1459056304", sheet = "Not Published")

unpublished_data$Year <- str_match_all(unpublished_data$`Dataset Title`, "[0-9]+")
  
unpublished_data <- unpublished_data %>% 
  mutate(Vessel = if_else(Year == "2019", "Professor Kaganovsky", "Pacific Legacy NO 1"),
         Vessel = if_else(Year == 'c("2019", "2020")', "Professor Kaganovsky, Pacific Legacy NO 1", Vessel),
         Year = as.numeric(ifelse(Year == 'c("2019", "2020")', "20192020", Year))) %>% 
  select(`Dataset Title`, Year, Vessel, `Contact Person`:Status, -`Metadata Link`)

```

```{r read in 2022 data}
twenty2 <- read_sheet('https://docs.google.com/spreadsheets/d/1grJN_xPZScd4r-IgUK7v_jnCs5IGKhDDElTskt6qdAQ/edit#gid=1020959877', sheet = 'Master Sheet') %>%
  mutate(Year = 2022) %>% 
  select(`Dataset Title` = 'Dataset Title / Description', Vessel, Year, `Contact Person` = 'Principal Investigator(s)', priority = 'Priority Category (For Brett, Tim and Caroline to fill out)', `Digital Object Identifier`, `Access Constraints` = `Limitations of data`, Status = 'Dataset Publication Status (For Aidan, Brett, Tim to fill out) (1-5 as per IYS About page dataset summary table)')

twenty2$`Contact Person` <- gsub("^(.*?),.*", "\\1", twenty2$`Contact Person`)
twenty2$`Contact Person` <- gsub("^(.*?)/.*", "\\1", twenty2$`Contact Person`)

scientists <- read_sheet('https://docs.google.com/spreadsheets/d/1grJN_xPZScd4r-IgUK7v_jnCs5IGKhDDElTskt6qdAQ/edit#gid=1020959877', sheet = 'Scientists') %>% 
  select('Name', 'Contact email')

twenty2 <- left_join(twenty2, scientists, by = c("Contact Person" = 'Name'))
```

```{r join 2022 with 2019 and 2022 tables}
published_22 <- twenty2 %>% 
  filter(Status > 2) %>% 
  rename(`Link to Access Data` = `Digital Object Identifier`) %>% 
  select(-priority) %>% 
  mutate(FAIRness = if_else(Status == 5, "FAIR", ""))

published_data <- bind_rows(published_data, published_22)

available_internally22 <- twenty2 %>% 
  filter(Status == 2)%>% 
  rename(`Link to Access Data` = `Digital Object Identifier`) %>% 
  select(-priority)  

internal_data <- bind_rows(internal_data, available_internally22) %>% 
  mutate(`Access Constraints` = "Internal access only")

unavailable22 <- twenty2 %>% 
  filter(Status %in% c(NA, 0, 1)) %>% 
  select(-priority)

unpublished_data <- bind_rows(unpublished_data, unavailable22) %>% 
  select(-`Digital Object Identifier`, `Access Constraints`)

all_data <- bind_rows(published_data, internal_data, unpublished_data)
```

# Available Data {.tabset .tabset-pills}

## Summary of Data Holdings

Last updated: `r Sys.Date()`

```{r summary of published data, include = TRUE, fig.cap= "Fig 1. The number of datasets received from the 2019 (Kagaonovsky), 2020 (Pacific Legacy) and 2022 International Year of the Salmon Expeditions."}

library(scales)
plot_data <- all_data |> 
  filter(Status >= 2,
         !Vessel %in% c("Argo Floats", "Glider", "Professor Kaganovsky, Pacific Legacy NO 1", "TINRO, Shimada, Franklin")) |>
  add_row(Vessel = "Professor Kaganovsky") |> 
  add_row(Vessel = "Professor Kaganovsky") |> 
  add_row(Vessel = "Pacific Legacy NO 1") |>
  add_row(Vessel = "Pacific Legacy NO 1") |>
  add_row(Vessel = "TINRO") |> 
  add_row(Vessel = "Shimada") |> 
  add_row(Vessel = "Franklin") |> 
  count(Vessel) |> 
  add_row(Vessel = "Raw Spirit", n = 0) # Remove if Raw Spirit data are added



plot_data$Vessel <- recode(plot_data$Vessel, `Professor Kaganovsky` = "Kaganovsky", `Pacific Legacy NO 1` = "Pacifc Legacy", `Northwest Explorer` = "NW Explorer")
  
ggplot(plot_data, aes(x = reorder(Vessel, n, decreasing = TRUE), y = n, fill = Vessel)) +
  geom_bar(stat = "identity") +
  xlab("Vessel") +
  theme_bw()+
  scale_y_continuous(breaks= pretty_breaks()) +
  theme(legend.position="none") +
  ylab("Numner of Published Datasets")

# 2022 Priority Data
```


```{r summary of unpublished priority 2022 data, include = TRUE, fig.cap= "Figure 2. The number of datasets from the 2022 IYS Expeditions in the 'At sea observations / Data reported in Cruise Report' Data Category defined in the IYS Data Policy either received or not received organized by vessel. Green colour indicates received, red indicates not received. Hover over each block to identify the dataset."}
library(plotly)
tt_nr <- twenty2 |> filter(priority == 1,
                  !Vessel %in% c("Argo Floats", "Glider")) |> 
  select(`Dataset Title`, Vessel, Status) |> 
  mutate(Status = if_else(Status < 2, "Not Received", "Received")) |> 
  add_count(Vessel, `Dataset Title`) 

n_published <- tt_nr |> 
  select(Vessel, Status) |> 
  filter(Status == "Received") |> 
  count(Vessel) |> 
  rename(n_published = n)

tt_nr <- left_join(tt_nr, n_published, by = "Vessel") |> 
  mutate(Vessel = reorder(Vessel, n_published, decreasing = TRUE))

colours <- c("red", "green")

tt_nr_plot <- ggplot(tt_nr, aes(x = Vessel, y = n, fill = Status, colour = `Dataset Title`)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(breaks= pretty_breaks()) +
  scale_fill_manual(values = colours) + 
  xlab('Vessel') +
  ylab("Number of Datasets")

tt_nr_plot <- tt_nr_plot +  theme(legend.position="none") 

ggplotly(tt_nr_plot)
```



## Published Data

Table 1. Data that have been published and are publicly available. 
```{r, include = TRUE}

reactable(published_data, 
          columns = list(
          `Link to Access Data` = colDef(html = TRUE, cell = JS("
    function(cellInfo) {
      // Render as a link
      const url = cellInfo.value
      return '<a href=\"' + url + '\" target=\"_blank\">' + cellInfo.value + '</a>'
    }
  "))),
          searchable = TRUE, resizable = TRUE, defaultSorted = list(Status = 'desc'))
```

## Data Available Internally



#### **IMPORTANT: To access internal data you must:**

* Register for a GitHub account https://github.com/signup
* Email iys.data@hakai.org your GitHub username (so we can add you as an approved user)
* Log in to GitHub before clicking on links below
* Offer data providers with collaboration opportunity according the the [IYS Data Policy](https://international-year-of-the-salmon.github.io/about/Final%20IYS%20Data%20Policy_Dec132021.pdf)

Table 2. Data that are not yet available publicly but are available internally.

```{r internal data reactable, include = TRUE}
reactable(internal_data,
          columns = list(
          `Link to Access Data` = colDef(html = TRUE, cell = JS("
    function(cellInfo) {
      // Render as a link
      const url = cellInfo.value
      return '<a href=\"' + url + '\" target=\"_blank\">' + cellInfo.value + '</a>'
    }
  "))),
  searchable = TRUE, resizable = TRUE, 
          defaultSorted = list(Status = 'desc'))
```

## Number of Downloads and Citations

```{r include = TRUE}
# Loading required packages
library(rgbif)
library(dplyr)
library(purrr)

# Searching for datasets related to "International Year of the Salmon"
iys_datasets <- datasets(query = "International Year of the Salmon")

# Extracting dataset data
iys_data <- iys_datasets$data

# Extracting dataset keys
iys_uuids <- iys_data$key

# Function to fetch literature search results for a given datasetKey
# and append datasetKey as a new column 'iys_pub' for citation tracking
fetch_citations <- function(key) {
  lit_search(datasetKey = key)$data %>% 
    mutate(iys_pub = key)
}

# Apply fetch_citations function to all uuids
# bind_rows function combines all fetched data frames into a single one
iys_lit_citations <- map_df(iys_uuids, fetch_citations)

# Preparing a dataframe with 'key', 'doi', and 'IYS_dataset_title' columns
# Adding a new column 'iys_doi' which combines "https://doi.org/" with 'doi'
iys_df <- iys_data %>% 
  select(key, doi, IYS_dataset_title = title) %>% 
  mutate(iys_doi = paste0("https://doi.org/", doi))

# Selecting relevant columns from iys_lit_citations
# Performing a left_join with iys_df dataframe to include dataset title and doi from IYS pubs
iys_lit_citations <- iys_lit_citations %>% 
  select(title, source, websites, year, iys_pub) %>% 
  left_join(iys_df, by = c("iys_pub" = 'key'))

# Summarizing citation data by counting number of citations for each IYS dataset
iys_citations_summary <- iys_lit_citations %>% 
  group_by(IYS_dataset_title, iys_pub) |> 
  summarize(count = n())

base_url <- "https://api.gbif.org/v1/occurrence/download/statistics/export?datasetKey="

download_and_read <- function(uuid) {
  url <- paste0(base_url, uuid)
  df <- read_tsv(url)
  return(df)
}

# Use purrr's map_df function to apply download_and_read to all uuids and bind the results
master_df <- map_df(iys_uuids, download_and_read) 
Sys.sleep(20)
master_df_summary <- master_df |> 
  group_by(dataset_key) |> 
  summarize(count = sum(number_downloads)) |>
  right_join(iys_citations_summary, by = c("dataset_key" = "iys_pub")) |> 
  select(IYS_dataset_title, downloads = count.x, citations = count.y)

reactable(master_df_summary)
```

# Fetch all citations 
```{r}
library(rdatacite)
# Using Datacite not just GBIF

iys_dois <- dc_dois(client_id = 'hakai.dfukgl', limit = 1000)

doi_data <- iys_dois[["data"]]
filtered_data <- doi_data %>% 
  filter(attributes$publisher == "North Pacific Anadromous Fish Commission") |> 
  select(id)

# Use list of IYS DOIs to retrieve citations
iys_dois <- dc_dois(ids = filtered_data$id, limit = 1000)

n_citations <- sum(iys_dois[["meta"]][["citations"]][["count"]])
```

```{r network of citations}
library(networkD3)
# Get the data from 'International Year of the Salmon' titles
iys_dois <- dc_dois(query = "titles.title:International Year of the Salmon", limit = 1000)

# Create a tibble with the title, citation count, and DOI for each record, then filter by citation count greater than 0
iys_citations <- tibble(
  title = lapply(iys_dois$data$attributes$titles, "[[", "title"),
  citations = iys_dois[["data"]][["attributes"]][["citationCount"]],
  doi = iys_dois[["data"]][["attributes"]][["doi"]]
) |> filter(citations > 0)

# Reduce the title to the substring from the 4th to the 80th character
iys_citations$title <- substr(iys_citations$title, 4, 80)

# Initialize a list to store citation details of each DOI
cites_iys_list <- list()

# Fetch citation details of each DOI and store it in the list
for (i in iys_citations$doi) {
  x <- dc_events(obj_id = paste0("https://doi.org/", i))
  cites_iys_list[[i]] <- x
}

# Initialize lists to store objId and subjId
obj_ids <- list()
subj_ids <- list()

# Loop over the list to retrieve objId and subjId
for(i in 1:length(cites_iys_list)) {
  data <- cites_iys_list[[i]]$data$attributes
  obj_ids[[i]] <- data$objId
  subj_ids[[i]] <- data$subjId
}

# Flatten the lists and remove the prefix 'https://doi.org/'
obj_ids <- substring(unlist(obj_ids), 17)
subj_ids <- substring(unlist(subj_ids), 17)

# Get titles for objId and subjId
obj_titles <- rdatacite::dc_dois(ids = obj_ids, limit = 1000)
subj_titles <- rdatacite::dc_dois(ids = subj_ids, limit = 1000)

# Create a tibble of position, obj_doi, and its corresponding title
obj_dois <- obj_titles[["data"]][["attributes"]][["doi"]]
title_list <- obj_titles[["data"]][["attributes"]][["titles"]]
title_vector <- unlist(map(title_list, function(x) x[['title']][1]))
seq <- as.character(1:length(obj_dois))
objects <- tibble(position = seq, obj_dois, title_vector)

# Create a tibble of position, subj_doi, and its corresponding title
subj_dois <- subj_titles[["data"]][["attributes"]][["doi"]]
subjtitle_list <- subj_titles[["data"]][["attributes"]][["titles"]]
subjtitle_vector <- unlist(map(subjtitle_list, function(x) x[['title']][1]))
seq2 <- as.character(1:length(subj_dois))
subjects <- tibble(seq2, subj_dois, subjtitle_vector) |> 
  filter(subjtitle_vector != "Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions")

# Get related identifiers and filter by obj_dois, join with subjects, and filter by relationType
subj_related_ids <- bind_rows(subj_titles[["data"]][["attributes"]][["relatedIdentifiers"]], .id = "position") |> 
  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |> 
  left_join(subjects, by = c('position' = 'seq2')) |> 
  filter(relationType != "IsPreviousVersionOf")

# Join objects and subj_related_ids by 'obj_dois' = 'relatedIdentifier'
relationships <- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))

# Prepare data for network plot
library(networkD3)

objects$type.label <- "IYS Dataset"
subjects$type.label <- "Referencing Dataset"
ids <- c(objects$obj_dois,subjects$subj_dois)
names <- c(objects$title_vector, subjects$subjtitle_vector)
type.label <- c(objects$type.label, subjects$type.label)

# Create edges for network plot
edges <-tibble(from = relationships$obj_dois, to = relationships$subj_dois)
links.d3 <- data.frame(from=as.numeric(factor(edges$from))-1, 
                       to=as.numeric(factor(edges$to))-1 ) 
size <- links.d3 |> 
   group_by(from) |> 
  summarize(weight = n())

nodes <- tibble(ids, names, type.label) |> 
  mutate(names = case_when(
    names == "Occurrence Download" ~ paste0(names, " ", ids),
    TRUE ~ names
    ),
  )

length <- nrow(nodes)

missing_length <- as.integer(length) - nrow(size)
missing_size <- rep.int(0, missing_length)
size <- c(size$weight, missing_size)

nodes$size <- size

nodes.d3 <- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) 

# Create and render the network plot
library(networkD3)
plot <- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source="from", Target="to",
               NodeID = "idn", Group = "type.label",linkWidth = 1,
               linkColour = "#afafaf", fontSize=12, zoom=T, legend=T, 
               Nodesize="size", opacity = 0.8, charge=-300,
               width = 600, height = 400)

plot
```

